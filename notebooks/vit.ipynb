{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-11-19T16:42:34.751411Z",
     "start_time": "2025-11-19T16:42:31.839858Z"
    }
   },
   "source": [
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torch.optim\n",
    "from models import MyVisionTransformer"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T16:42:34.782212Z",
     "start_time": "2025-11-19T16:42:34.755470Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Dataset loading\n",
    "folder_train = torchvision.datasets.ImageFolder(\n",
    "    root=\"../train_images\",\n",
    "    transform=torchvision.transforms.Compose([\n",
    "        torchvision.transforms.Resize((224, 224)),\n",
    "        torchvision.transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.2),\n",
    "        torchvision.transforms.RandomGrayscale(0.25),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "    ])\n",
    ")\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    folder_train,\n",
    "    batch_size=32,\n",
    "    shuffle=True\n",
    ")"
   ],
   "id": "f3b0c41e67747a47",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T16:42:34.953447Z",
     "start_time": "2025-11-19T16:42:34.786291Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = MyVisionTransformer().to(device)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.003)\n",
    "model.train()"
   ],
   "id": "b0035530efde9c7",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Programming\\PJPProject\\BIA\\.venv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MyVisionTransformer(\n",
       "  (patch_layer): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "  (transformer): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-2): 3 x TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classification_head): Sequential(\n",
       "    (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (1): Linear(in_features=768, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T16:52:00.209798Z",
     "start_time": "2025-11-19T16:42:39.221277Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Train\n",
    "num_episodes = 20\n",
    "\n",
    "best_loss = float(\"inf\")\n",
    "for episode in range(num_episodes):\n",
    "    episode_loss = 0\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        # Preprocess\n",
    "        images, labels = images.to(device), labels.float().to(device).view(-1, 1)\n",
    "\n",
    "        # Reset gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Feed stuff into model\n",
    "        outputs = model(images)\n",
    "\n",
    "        # Compute loss and backpropagate\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate loss\n",
    "        episode_loss += loss.item()\n",
    "\n",
    "    print(f\"Episode {episode}, loss: {episode_loss}\")\n",
    "\n",
    "    if episode_loss < best_loss:\n",
    "        best_loss = episode_loss\n",
    "        torch.save(model.state_dict(), \"../models/parking_vit.pth\")\n",
    "        print(\"\\tSaving!\")\n"
   ],
   "id": "7fdd03e66c35c6c0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0, loss: 90.18940916657448\n",
      "\tSaving!\n",
      "Episode 1, loss: 76.59919267892838\n",
      "\tSaving!\n",
      "Episode 2, loss: 67.10758204758167\n",
      "\tSaving!\n",
      "Episode 3, loss: 54.75754349672934\n",
      "\tSaving!\n",
      "Episode 4, loss: 14.683990838937461\n",
      "\tSaving!\n",
      "Episode 5, loss: 6.550026143086143\n",
      "\tSaving!\n",
      "Episode 6, loss: 6.308090866135899\n",
      "\tSaving!\n",
      "Episode 7, loss: 6.365665448887739\n",
      "Episode 8, loss: 7.172850830946118\n",
      "Episode 9, loss: 7.295977511093952\n",
      "Episode 10, loss: 4.893221385427751\n",
      "\tSaving!\n",
      "Episode 11, loss: 6.236641188152134\n",
      "Episode 12, loss: 4.243655933620175\n",
      "\tSaving!\n",
      "Episode 13, loss: 5.183591270353645\n",
      "Episode 14, loss: 4.541741847642697\n",
      "Episode 15, loss: 6.642425086960429\n",
      "Episode 16, loss: 5.187232234922703\n",
      "Episode 17, loss: 5.011467203410575\n",
      "Episode 18, loss: 3.505121786009113\n",
      "\tSaving!\n",
      "Episode 19, loss: 3.0266292620799504\n",
      "\tSaving!\n"
     ]
    }
   ],
   "execution_count": 4
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
